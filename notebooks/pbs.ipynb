{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed as dd\n",
    "from dask.distributed import Client, LocalCluster, progress\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import get_worker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f5d4b-8ccb-45e7-8559-8a16315930a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the workshop directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce81220-5ea8-4f22-be65-c75a58a7de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the python we use is from the venv\n",
    "\n",
    "os.environ['DASK_PYTHON'] = '/g/data/vp91/Training-Venvs/intro-to-dask/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c8a3c-3c8a-4d3d-9e07-2ea51cafb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all the modules are loaded.\n",
    "# It is essential that we use the same python and library for all aspects of dask\n",
    "# If we dont activate the venv then the workers may have a different versions of libraries\n",
    "\n",
    "setup_commands = [\"module load python3/3.11.0\", \n",
    "                  \"source /g/data/vp91/Training-Venvs/intro-to-dask/bin/activate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e2dbb-4a9a-46a0-89d1-6076fbe0f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gadi use custom PBS directives\n",
    "# So some of the default values to launch a PBS job through Dask call will not work in Gadi\n",
    "# Any directive specific to gadi should be mentioned here.\n",
    "# refer : https://opus.nci.org.au/display/Help/Gadi+Quick+Reference+Guide\n",
    "extra = ['-q normal',\n",
    "         '-P vp91', \n",
    "         '-l ncpus=48', \n",
    "         '-l mem=192GB',\n",
    "         '-l storage=scratch/vp91+gdata/vp91']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# walltime: Walltime for each worker job.\n",
    "# cores: Total number of cores per job.\n",
    "# shebang: Path to desired interpreter for your batch submission script.\n",
    "# job_extra_directives: List of other PBS options. Each option will be prepended with the #PBS prefix.\n",
    "# local_directory: Dask worker local directory for file spilling.\n",
    "# job_directives_skip: Directives to skip in the generated job script header. Directives lines containing \n",
    "#                      the specified strings will be removed. Directives added by job_extra_directives \n",
    "#                      won’t be affected.\n",
    "# interface: Network interface like ‘eth0’ or ‘ib0’. This will be used both for the Dask scheduler and \n",
    "#            the Dask workers interface\n",
    "# job_script_prologue: Commands to add to script before launching worker\n",
    "# python: Python executable used to launch Dask workers. Defaults to the Python that is submitting these jobs\n",
    "\n",
    "\n",
    "\n",
    "cluster = PBSCluster(walltime=\"00:20:00\", \n",
    "                     cores=48, \n",
    "                     memory=\"192GB\",\n",
    "                     shebang='#!/usr/bin/env bash',\n",
    "                     job_extra_directives=extra, \n",
    "                     local_directory='$TMPDIR', \n",
    "                     job_directives_skip=[\"select\"], \n",
    "                     interface=\"ib0\",\n",
    "                     job_script_prologue=setup_commands,\n",
    "                     python=os.environ[\"DASK_PYTHON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1811896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cluster with 2 nodes\n",
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf9b63-5a88-4958-b2df-3e9645a0a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the workers have been allocated as expected\n",
    "!qstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76568e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74475a75-117e-46f4-ad0c-3866824de73c",
   "metadata": {},
   "source": [
    "cluster.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple test function\n",
    "def slow_increment(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a413b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the work to the Dask cluster\n",
    "futures = client.submit(slow_increment, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b436a-1f7c-4d2d-b6ae-1776f2c100e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f742e-4a34-486c-aa42-575dd7202901",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ac295-06d7-4249-bd36-f5f38e0ec7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as df\n",
    "\n",
    "ddf = df.read_csv(\n",
    "    os.path.join(\"intro-to-dask/data\", \"nycflights\", \"*.csv\"),\n",
    "    dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97e8fa-de89-443b-8219-aafc3cc95fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b068f-e479-4e08-83a6-88aa854043f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Create two large random arrays\n",
    "a = da.random.random((20000, 20000), chunks=(2000, 2000))\n",
    "b = da.random.random((20000, 20000), chunks=(2000, 2000))\n",
    "\n",
    "# Matrix multiplication (this creates a large graph of tasks)\n",
    "c = da.dot(a, b)\n",
    "\n",
    "# Trigger computation\n",
    "result = c.mean().compute()\n",
    "\n",
    "print(\"Mean of result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0bf08-5c4f-4183-a4aa-f213cafc1b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
