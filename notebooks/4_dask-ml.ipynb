{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a154485",
   "metadata": {},
   "source": [
    "# This tutorial introduces the concepts of Machine Learing Using Dask.\n",
    "\n",
    "Learning outcomes of the tutorial are:\n",
    "\n",
    "1. Learn how to do data prepocessing.\n",
    "2. Learn how to implement a linear regression model.\n",
    "3. Learn how to implement a K-Means clustering Model.\n",
    "4. Learn how to cross validate a model.\n",
    "5. Learn how to build ML pipelines.\n",
    "\n",
    "Prerequisite:\n",
    "\n",
    "1. Experience with Scikit Learn library\n",
    "2. Experience with Dask Dataframe and Dask Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e790fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline  # regular scikit-learn pipeline\n",
    "\n",
    "import dask \n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder, StandardScaler, MinMaxScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LogisticRegression, LinearRegression\n",
    "from dask_ml.decomposition import PCA\n",
    "from dask_ml.cluster import KMeans\n",
    "\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fab132-51b1-4c61-89ee-4331061c8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the workshop directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package\n",
    "\n",
    "ddf = dd.read_csv(\"intro-to-dask/data/weather/weatherAUS*.csv\", dtype={'Humidity3pm': 'float64',\n",
    "       'Humidity9am': 'float64',\n",
    "       'WindGustSpeed': 'float64',\n",
    "       'WindSpeed3pm': 'float64',\n",
    "       'WindSpeed9am': 'float64'})\n",
    "ddf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0799cef",
   "metadata": {},
   "source": [
    "# Data Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201e4bb",
   "metadata": {},
   "source": [
    "The first process step in building a machine learning model is data cleaning. The data we have here is not very complex which makes data cleaning easier. In the production quality ML model, this is the most time-consuming step. \n",
    "\n",
    "Data cleaning mainly involves:\n",
    "1. Remove any unnecessary observations from your dataset\n",
    "2. Remove redundant information\n",
    "3. Remove duplicate information\n",
    "4. Remove structural errors in data collection\n",
    "5. Remove unwanted outliers - outliers can result in overfitting\n",
    "6. Handle missing data:\n",
    "    * Remove observations with values missing\n",
    "    * Infer the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2de6f3",
   "metadata": {},
   "source": [
    "In this case, we are taking the easiest method to address missing values. We are removing any dataframe row that has missing values. This is not always advisable as we are losing a lot of information and in the end, we end up not getting the entire picture.\n",
    "\n",
    "Inferring data is also not always a good idea as we may add some bias to the inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a42c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_clean = ddf.dropna() \n",
    "ddf_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = ddf_clean.shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52061250",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(shape[0]))\n",
    "print(type(shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fa0a7",
   "metadata": {},
   "source": [
    "As you can see the columns are immediately computed while the rows are not. We have invoked compute get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b390831",
   "metadata": {},
   "source": [
    "Here, we are trying to predict the temperature based at 3PM.\n",
    "1. We divide the data frame into target and features.\n",
    "    * Target is the value we are trying to predict\n",
    "    * Feature are the data points used to predict the target\n",
    "2.  We remove all the features we deem unncessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa695e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "ddf_target = ddf_clean['Temp3pm']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278e04",
   "metadata": {},
   "source": [
    "Data usually have numerical data and categorical data. \n",
    "1. Categorical data groups information (usually text) with similar characteristics \n",
    "2. Numerical data expresses information in the form of numbers\n",
    "\n",
    "Most machine learning algorithms cannot handle categorical variables unless it is converted to numerical data. This process is called encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb58db",
   "metadata": {},
   "source": [
    "Ideally, all categorical data should be converted to numerical data. In this case, we remove all catogorical data other than 'RainToday' and 'RainTomorrow'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182791c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "ddf_features = ddf_clean.drop(columns=['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Temp3pm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb11421",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = ddf_features.shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a84156",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28afd14",
   "metadata": {},
   "source": [
    "There are two types of categorical data in Dask\n",
    "1. Known: categories are known statically (from the metadata)\n",
    "2. Unknown: categories are not known statically (from the metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2858b4",
   "metadata": {},
   "source": [
    "**categorize()** function in meta scans the entire data find the different catogories in a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features = ddf_features.categorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7764c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9612ee3",
   "metadata": {},
   "source": [
    "We can verify if the catogries of a feature are known as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff830bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features.RainTomorrow.cat.known"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85fd4d",
   "metadata": {},
   "source": [
    "Encoding is the method of converting categorical values into numerical values (and vice versa). There are two here we use **Dummy Encoding**. Each category end up getting a binary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac78040",
   "metadata": {},
   "outputs": [],
   "source": [
    "de = DummyEncoder()\n",
    "ddf_features_preproc = de.fit_transform(ddf_features.categorize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11232d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features_preproc.head().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c16f3-d0cb-41f9-98f9-4488a215a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features_preproc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ed694-beb2-4673-984f-d895c9bf4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool_cols = ddf_features_preproc.select_dtypes(include='bool').columns\n",
    "for col in ddf_features_preproc.select_dtypes(include='bool').columns:\n",
    "    ddf_features_preproc[col] = ddf_features_preproc[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c232f-b97f-4b71-9a9c-3b30f8658d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features_preproc.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd402b-1669-4cc7-a441-dd85ae9b9b38",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "Data standardization becomes relevant when there are substantial variations in the ranges of features within the input dataset or when those features are measured using different units (meteres, kilogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc2220-b37f-45ef-95ef-1ebfe9735b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scalar_std = scaler.fit(ddf_features_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530f8a5-ad5c-47fc-bb3d-5541d819cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_std.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b0ae9-f761-488e-8b1d-c7f15c8674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features_std = scaler.transform(ddf_features_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d28b4f-f30b-4677-9e3d-73873d81967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a8ddf-c75f-4034-9ea1-b5e6a9fb1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization can result in NaN values. So check that.\n",
    "na_count = ddf_features_std.isna().sum().compute()\n",
    "\n",
    "# Output the result\n",
    "print(na_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ec93b",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization is the process of translating data into a range. It is a good practice to normalize the data - especially useful when different features have different value ranges. Normalization ensures that one feature does not overtly influence the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26614b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMax.fit(ddf_features_std)                          # Fit once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45371fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_features_norm = MinMax.transform(ddf_features_std)  # Then transform\n",
    "ddf_features_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e76a62-08ea-4556-b924-21545b785186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization can result in NaN values. So check that.\n",
    "na_count = ddf_features_norm.isna().sum().compute()\n",
    "\n",
    "# Output the result\n",
    "print(na_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4675716",
   "metadata": {},
   "source": [
    "## Correlation Matrix\n",
    "\n",
    "Correlation is often used in machine learning to identify multicollinearity, which is when two or more predictor variables are highly correlated with each other. Multicollinearity can adversely affect the accuracy of predictive models.\n",
    "\n",
    "* The coefficients become very sensitive to small changes in the model.\n",
    "* Multicollinearity reduces the precision of the estimated coefficients, which weakens the statistical power of your regression model. \n",
    "\n",
    "Multicollinearity can be addressed by removing one of the correlated variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeeba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = ddf_features_norm.corr(method='pearson', min_periods=None, numeric_only='__no_default__', split_every=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18da81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr_matrix, annot=True, mask = mask, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ae42d",
   "metadata": {},
   "source": [
    "Ideally we should remove one of the higly correlated feature or combine those together. For the time being we doing neither. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61add025",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit PCA\n",
    "pca.fit(ddf_features_norm.to_dask_array(lengths=True))\n",
    "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None, svd_solver='auto', tol=0.0, whiten=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and apply the principal components\n",
    "# This gives you a Dask array of shape (n_samples, 3) with the data projected onto the principal components\n",
    "\n",
    "X_pca = pca.fit_transform(ddf_features_norm.to_dask_array(lengths=True))\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c94469",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de928e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c14f9-7d0b-4d73-9ee9-edf89e2db322",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118c433-3c30-43d0-b04f-7a8d3461b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optionally, get column names from PCA\n",
    "column_names = [f\"PC{i+1}\" for i in range(pca.n_components)]\n",
    "print(column_names)\n",
    "\n",
    "# Create a Dask DataFrame\n",
    "ddf_pca = dd.from_dask_array(X_pca, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ba261-5d9c-4616-a799-e426210ee352",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52329f-000c-4600-a095-6fc56c561994",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37121591-2689-4bc3-9e8d-629c80cca375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization can result in NaN values. So check that.\n",
    "na_count = ddf_features_norm.isna().sum().compute()\n",
    "\n",
    "# Output the result\n",
    "print(na_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcebdc7",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc38a46-cee6-4b38-9178-280b85c730ba",
   "metadata": {},
   "source": [
    "We divide the dataset into training set and testing set. Training set is used to train the model, while the testing set will be used to measure the accuracy of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ba95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into learning and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ddf_features_norm, ddf_target, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.compute().head()\n",
    "X_test.compute().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ca3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.compute().head()\n",
    "y_test.compute().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b36341",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = y_train.to_dask_array(lengths=True)\n",
    "X_train_array = X_train.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c860d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63dcfa",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression is used to predict the value of a variable based on the value of another variable or a set of varibales. It a type of **Supervised Learning**. Supervised machine learning involves the process of establishing a connection between input variables and output variables. The input variabls are often called features or independent variables, while the output is commonly denoted as the target or 'dependent variables. Data containing both these features and the target is typically termed labeled data.\n",
    "\n",
    "Linear regression tries to find the optimal W<sub>1</sub>, W<sub>2</sub>, W<sub>3</sub>, W<sub>4</sub>, so that we can predict the value of Y for the user-supplied X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>.\n",
    "\n",
    "$$\n",
    "  Y(X_1, X_2, X_3) = W_1 * X_1 + W_2 * X_2 + W_3 * X_3 + W_4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99fdea-b500-4349-9678-57488385cf8d",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_array, y_train_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = X_test.to_dask_array(lengths=True, chunks=(1000, X_test.shape[1]))  # 1000 samples per chunk, full features\n",
    "y_test_array = y_test.to_dask_array(lengths=True, chunks=(1000,))  # 1000 samples per chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb1cc7-4e74-4825-adb9-ddfdcb6329b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdb1f2-fbcb-47d5-a94e-9ad255796221",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = lr.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d788197-5dd7-4580-9747-5aae292666ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0].compute())\n",
    "print(y_test_array[0].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbad0db-1c20-4c7f-ad03-4c031913ab8d",
   "metadata": {},
   "source": [
    "### Score the performance of the model using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba50e2-9e9c-4166-816c-46447f50bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test_array.compute(), y_pred.compute())\n",
    "\n",
    "print(\"Accuracy = \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc913064-eeda-4390-8793-00dd8a4b0a89",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "Cross-validation is a method for evaluating ML models by training several ML models on subsets of the data and evaluating another subset of the data. The advantages of cross validation are : \n",
    "\n",
    "1. Identify Overfitting\n",
    "2. Comparison between different models \n",
    "3. Hyperparameter tuning\n",
    "4. Efficiency : Allows the use of data for both training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e99f3-a3b3-4abd-aba5-aa5750009c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation (e.g., 5-fold)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "# Define a (dummy) param grid; even if you don't want to tune, it's needed\n",
    "param_grid = {\n",
    "    # LinearRegression has no real hyperparams; this is just for structure\n",
    "    # Add any other model params here if applicable\n",
    "}\n",
    "\n",
    "# Cross-validator (5-fold CV)\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=cv, scoring='r2')\n",
    "grid.fit(X_train_array, y_train_array)\n",
    "\n",
    "# Best score and estimator\n",
    "print(\"Best RÂ² score:\", grid.best_score_)\n",
    "print(\"Best estimator:\", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2813fc6-c9cf-4f2b-a6a7-852dcbf0b164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00313b20-52b8-472b-b86a-d853bf82a82d",
   "metadata": {},
   "source": [
    "| Aspect               | `cross-validation`                    | `.score(...)`                         |\n",
    "| -------------------- | ------------------------------------- | ------------------------------------- |\n",
    "| Data splits          | Multiple (k-folds)                    | Single                                |\n",
    "| Trains model?        | Yes, multiple times                   | No (uses already fitted model)        |\n",
    "| Gives variance?      | Yes (scores per fold)                 | No                                    |\n",
    "| Purpose              | Model validation/generalization check | Evaluate performance on specific data |\n",
    "| Risk of overfitting? | Lower                                 | Higher if only tested on one split    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068451a",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "k-means clustering partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid). k-means clustering is a type of **Unsupervised Learning**. In unsupervised learning the algorithm groups or patterns without the need of labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ddf = dd.read_csv(\"/g/data/vp91/Training-Data/Diabetes/diabetes.csv\", dtype={\n",
    "        'Pregnancies': 'float64',\n",
    "        'Glucose': 'float64',\n",
    "        'BloodPressure': 'float64',\n",
    "        'SkinThickness': 'float64',\n",
    "        'Insulin': 'float64',\n",
    "        'BMI': 'float64',\n",
    "        'DiabetesPedigreeFunction': 'float64',\n",
    "        'Age': 'float64',\n",
    "        'Outcome': 'float64'})\n",
    "\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc4fdf-f6f5-4e6f-8cb0-b7e8e6ac9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed743364",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3964f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4873ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ddf = scaler.fit_transform(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e31ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ddf.compute(), x = 'BMI', y = 'DiabetesPedigreeFunction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72096cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, init_max_iter=1, oversampling_factor=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c3446-7868-4ee9-b470-1ae4d31fbaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ddf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b766491",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = ddf.map_partitions(len, meta=('x', int)).compute()\n",
    "X = ddf.to_dask_array(lengths=tuple(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3bc4d-049d-458b-bb28-643251de87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689009ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ddf.compute(), x = 'BMI', y = 'DiabetesPedigreeFunction', hue = kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31fed9",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Test the result without data normalization\n",
    "2. Apply other data preprocessing to the data\n",
    "3. Change the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c91a93",
   "metadata": {},
   "source": [
    "# Pipelining\n",
    "We saw that an ML workflow involves multiple stages. We can combine multiple stages of this workflow into a single pipeline. This is especially useful when your model is iterative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52115af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('reduce_dim', PCA()), \n",
    "                 ('cluster', KMeans(n_clusters = 3, random_state = 0, n_init='auto'))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e841f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = pipe.fit(ddf.to_dask_array(lengths=tuple(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ddf.compute(), x = 'BMI', y = 'DiabetesPedigreeFunction', hue = pipe['cluster'].labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9c521",
   "metadata": {},
   "source": [
    "# Excersice\n",
    "1. Add normalization to the pipeline (Solutions1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
